{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c8abbb7-f791-4de4-8b6b-954bb6c4aa6c",
   "metadata": {},
   "source": [
    "Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f28d85c3-3e44-4bfb-a53c-476caf53c8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ridge Regression is a model tuning Parameter which is used to reduce overfitting. It is also used to analyse data that suffers from multicollinearity\n",
    "## When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values being far away from the actual values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe0cab2f-5d35-4de6-929b-5e190a6345fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## here in Ridge Regression we add penalty term Lambda(slope)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5824b6e5-91ef-4f93-8313-0494d963f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## It shrinks the parameters. Therefore, it is used to prevent multicollinearity\n",
    "## It reduces the model complexity by coefficient shrinkage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2396c24a-c117-43c1-a79b-014f8cd7fd7d",
   "metadata": {},
   "source": [
    "Q2. What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "778fdd51-a183-4551-9892-3ca45bac817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The assumptions of ridge regression are the same as that of linear regression: linearity, constant variance, and independence.\n",
    "## However, as ridge regression does not provide confidence limits, the distribution of errors to be normal need not be assumed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de25465-7475-4d67-8c8e-1f0e45a1d047",
   "metadata": {},
   "source": [
    "Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c54ea1a8-0d8f-4cf4-89fd-9d0dc2ae07f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Selecting a good value for λ is critical.\n",
    "## When λ=0, the penalty term has no effect, and ridge regression will produce the classical least square coefficients. \n",
    "## However, as λ increases to infinite, the impact of the shrinkage penalty grows, and the ridge regression coefficients will get close zero.\n",
    "## We can use cross validation to select best lambda parameter "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b259b4-349e-4480-acca-13d99ebbb066",
   "metadata": {},
   "source": [
    "Q4. Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6281fcdc-0c0a-4a57-8e81-661474f9b684",
   "metadata": {},
   "outputs": [],
   "source": [
    "## No it cannot be used for feature selection because it shrinks the coefficients but does not reduce to 0, which was in case of lasso Regression.\n",
    "## It is used when multicollinearity is there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0790dd06-d507-4a8e-a798-cee9faedb0bf",
   "metadata": {},
   "source": [
    "Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50f4dd48-af79-4691-aa2d-11ff33361f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## It is useful since whenever multicollinearity happens in the data, the data exhibits large variances, and the least-squares will be unbiased.\n",
    "## Thus the predicted values and actual values will have large variances.\n",
    "## we add a lambda (slope)^2 to the cost function\n",
    "\n",
    "## If values of Lambda get bigger, the penalty is larger, and the coefficient’s magnitude is smaller. \n",
    "## Thus it can prevent multicollinearity by parameter-shrinking, and further model complexity is reduced due to the shrinkage of the coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c09c1ff-0c21-4d6f-a0a6-1daac918705b",
   "metadata": {},
   "source": [
    "Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c735a0d-155d-4e61-b38e-eccb78372fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Yes Ridge Regression handle both caegorical and continuous independent variables, we need to encode categorical variables using one-hot/label/nominal encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6680d091-95a5-43c0-a12f-1f53a0cea965",
   "metadata": {},
   "source": [
    "Q7. How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d289d678-ca9c-4ed8-8301-de85b323a9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The interpretation of the coefficients in Ridge Regression is similar to that of ordinary linear regression.\n",
    "## However, since Ridge Regression introduces a penalty term to the objective function, \n",
    "## the coefficients are generally smaller in magnitude compared to those obtained from ordinary linear regression. \n",
    "## Therefore, the coefficients should be interpreted in relation to the penalty parameter (alpha) used in Ridge Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2a0c9d3-75ff-444c-9c7d-fc90d55f5620",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A positive coefficient indicates that an increase in the corresponding predictor variable leads to an increase in the response variable,\n",
    "## while a negative coefficient indicates the opposite. \n",
    "## The magnitude of the coefficient indicates the strength of the relationship between the predictor variable and the response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24a7056-82ba-476a-b1ad-607dc218e3de",
   "metadata": {},
   "source": [
    "Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "125d7581-060b-4003-8e9d-6c2b2ce0a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Yes it can be used However, when working with time-series data, it is important to consider the autocorrelation (i.e., correlation between observations at different points in time)\n",
    "## in the data, which can violate the assumptions of independent and identically distributed errors made by Ridge Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd90ea83-e323-4df5-ad48-25e4265fc3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## One approach to using Ridge Regression for time-series data analysis is to incorporate lags of the response variable and predictors into the model. \n",
    "## This approach is known as autoregressive Ridge Regression (ARR), which extends the standard Ridge Regression framework by adding autoregressive terms to the model. \n",
    "## The autoregressive terms account for the autocorrelation in the data and help to improve the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbdb990-36bc-47e5-9be3-5119d9af8603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
